version: '3'

volumes:
  datanode:
  namenode:
  hadoop_historyserver:
  kafka:

networks:
  hbase:
    external:
      name: 'hbase'

services:

  # ----------------------------------------------------------------------------------------------------
  # ZooKeeper is a centralized service for maintaining configuration information, naming, providing
  # distributed synchronization, and providing group services. All of these kinds of services are
  # used in some form or another by distributed applications. Each time they are implemented there
  # is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because
  # of the difficulty of implementing these kinds of services, applications initially usually skimp
  # on them, which make them brittle in the presence of change and difficult to manage. Even when
  # done correctly, different implementations of these services lead to management complexity when
  # the applications are deployed.
  # ----------------------------------------------------------------------------------------------------
  zookeeper:
    image: 'wurstmeister/zookeeper'
    networks:
      - 'hbase'
    container_name: 'zookeeper'
    ports:
      - '2181:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  # ----------------------------------------------------------------------------------------------------
  # Kafka is an open-source distributed event streaming platform originally developed by LinkedIn,
  # now part of the Apache Software Foundation. It is designed to handle real-time data feeds, with
  # a focus on fault tolerance, high throughput, and low latency.
  #
  # At a high level, Kafka allows producers to write streams of records to a set of topics, which
  # are partitioned and distributed across a cluster of nodes. Consumers can then read from one or
  # more topics and process the records in real time. Kafka is horizontally scalable, meaning that
  # it can handle large volumes of data by adding more nodes to the cluster.
  # ----------------------------------------------------------------------------------------------------
  kafka:
    image: 'confluentinc/cp-kafka:7.3.2'
    hostname: 'kafka'
    container_name: 'kafka'
    networks:
      - 'hbase'
    ports:
      - '9092:9092'
      - '19092:19092'
      - '29092:29092'
    depends_on:
      - 'zookeeper'
    environment:
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka:29092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:19092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: 'kafka.controller=DEBUG,kafka.producer.async.DefaultEventHandler=DEBUG,state.change.logger=DEBUG'
      KAFKA_AUTHORIZER_CLASS_NAME: 'kafka.security.authorizer.AclAuthorizer'
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: 'true'
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    depends_on:
      - 'zookeeper'

  # ----------------------------------------------------------------------------------------------------
  # Kafka Schema Registry is a centralized service that manages the schema of messages sent through
  # Apache Kafka. It provides a way for producers and consumers to agree on a specific schema for
  # their data, which allows for interoperability between different systems and applications. The
  # Schema Registry ensures that all messages conform to the specified schema, which helps prevent
  # data inconsistencies and errors. It also allows for versioning of schemas, so that changes can
  # be made to the schema without breaking existing applications. The Schema Registry is commonly
  # used in conjunction with Apache Kafka and is a critical component of many real-time data
  # processing pipelines. Overall, the Kafka Schema Registry is an important tool for ensuring
  # data consistency and interoperability in complex data architectures.
  # ----------------------------------------------------------------------------------------------------
  kafka-schema-registry:
    image: 'confluentinc/cp-schema-registry:5.2.1'
    container_name: 'kafka-schema-registry'
    hostname: 'kafka-schema-registry'
    networks:
      - 'hbase'
    ports:
      - '8081:8081'
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'PLAINTEXT://kafka:29092'
      SCHEMA_REGISTRY_HOST_NAME: 'kafka-schema-registry'
      SCHEMA_REGISTRY_LISTENERS: 'http://0.0.0.0:8081'
    depends_on:
      - 'zookeeper'
      - 'kafka'

  # ----------------------------------------------------------------------------------------------------
  # Kafka Connect is a framework and set of APIs for integrating Kafka with external data sources
  # and sinks. It provides a scalable and fault-tolerant way to move data between Kafka and other
  # systems, such as databases, file systems, and message queues. Kafka Connect consists of two main
  # components: source connectors and sink connectors. Source connectors allow Kafka to consume data
  # from external systems and publish it to a Kafka topic, while sink connectors allow Kafka to push
  # data from a Kafka topic to an external system. Kafka Connect provides an easy-to-use interface
  # for configuring, deploying, and monitoring connectors. It also supports various transformation
  # and serialization formats, such as JSON, Avro, and Protobuf. Kafka Connect is commonly used in
  # data integration scenarios, such as replicating data between different databases or streaming
  # data from a web service to a Kafka topic. Overall, Kafka Connect is a valuable tool for building
  # scalable and flexible data pipelines that integrate with Apache Kafka.
  # ----------------------------------------------------------------------------------------------------
  kafka-connect:
    image: 'confluentinc/cp-kafka-connect:5.2.1'
    hostname: 'kafka-connect'
    container_name: 'kafka-connect'
    ports:
      - '8083:8083'
    networks:
      - 'hbase'
    volumes:
      - 'kafka:/etc/kafka-connect/jars/'
    depends_on:
      - 'zookeeper'
      - 'kafka'
      - 'kafka-schema-registry'
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka:29092'
      CONNECT_GROUP_ID: 'kafka-connect-my-topic'
      CONNECT_CONNECTOR_CLASS: 'com.cloudera.dim.kafka.connect.hdfs.HdfsSinkConnector'
      CONNECT_TASKS_MAX: 1
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST_NAME: 'kafka-connect'
      CONNECT_CONFIG_STORAGE_TOPIC: 'my-topic-config'
      CONNECT_OFFSET_STORAGE_TOPIC: 'my-topic-offsets'
      CONNECT_STATUS_STORAGE_TOPIC: 'my-topic-status'
      CONNECT_KEY_CONVERTER: 'io.confluent.connect.avro.AvroConverter'
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://kafka-schema-registry:8081'
      CONNECT_VALUE_CONVERTER: 'io.confluent.connect.avro.AvroConverter'
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://kafka-schema-registry:8081'
      CONNECT_VALUE_CONVERTER_PASSTHROUGH_ENABLED: 'true'
      CONNECT_TOPICS: 'my_topic'
      CONNECT_OUTPUT_WRITER: 'com.cloudera.dim.kafka.connect.hdfs.avro.AvroPartitionWriter'
      CONNECT_OUTPUT_AVRO_PASSTHROUGH_ENABLED: 'true'
      CONNECT_LOG4J_ROOT_LOGLEVEL: 'ERROR'
      CONNECT_LOG4J_LOGGERS: 'org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR'
      CONNECT_INTERNAL_KEY_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_INTERNAL_VALUE_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_HDFS_URI: 'hdfs://namenode:9870'
      CONNECT_HDFS_OUTPUT: '/my_topic_output/'

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, the NameNode is a key component of the Hadoop Distributed File System (HDFS).
  # It is responsible for managing the file system namespace and regulating access to files
  # by clients. The NameNode is a centralized component that runs on a dedicated machine in
  # the cluster, and it maintains the metadata about the files stored in HDFS, such as the
  # file name, directory structure, and the location of blocks that make up the file.
  #
  # The NameNode stores this metadata in memory for fast access, and it also persists it on
  # disk in the form of two files: fsimage and edits. The fsimage file contains a snapshot
  # of the file system metadata, and the edits file contains a log of all the changes that
  # have been made to the metadata since the last snapshot. Together, these files form a
  # checkpoint of the file system state that can be used to recover the metadata in case of a failure.
  #
  # When a client wants to read or write a file in HDFS, it first contacts the NameNode to
  # obtain information about the file, such as its location and the block IDs that make up
  # the file. The NameNode then returns this information to the client, which can then communicate
  # directly with the DataNodes that store the blocks.
  # ----------------------------------------------------------------------------------------------------
  namenode:
    image: 'bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8'
    container_name: 'namenode'
    ports:
      - '9870:9870'
      - '9000:9000'
    networks:
      - 'hbase'
    volumes:
      - 'namenode:/hadoop/dfs/name'
    environment:
      - 'CLUSTER_NAME=test'
    env_file:
      - './config/hadoop.env'

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, a DataNode is a component of the Hadoop Distributed File System (HDFS) that stores
  # the actual data in the form of blocks. The DataNode is responsible for reading and writing data
  # from the local file system, and for communicating with other DataNodes and the NameNode to manage
  # the data stored in the cluster.
  #
  # Each DataNode in the HDFS cluster stores a subset of the blocks that make up the files in the
  # file system. When a client wants to read or write a file, it first contacts the NameNode to
  # obtain the locations of the blocks that make up the file. The client can then read or write
  # the data directly from the DataNodes that store the blocks.
  #
  # DataNodes are designed to run on commodity hardware and can be added or removed from the cluster
  # as needed to scale the storage capacity of the HDFS cluster. The HDFS architecture is designed
  # to be fault-tolerant, so when a DataNode fails or becomes unavailable, the NameNode automatically
  # replicates the blocks that were stored on the failed DataNode to other DataNodes in the cluster
  # to ensure that the data is still available.
  # ----------------------------------------------------------------------------------------------------
  datanode1:
    image: 'bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8'
    container_name: 'datenode1'
    depends_on:
      - 'namenode'
    ports:
      - '9864:9864'
    networks:
      - 'hbase'
    volumes:
      - 'datanode:/hadoop/dfs/data1'
    env_file:
      - './config/hadoop.env'
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
  datanode2:
    image: 'bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8'
    container_name: 'datenode2'
    ports:
      - '9865:9864'
    networks:
      - 'hbase'
    volumes:
      - 'datanode:/hadoop/dfs/data2'
    env_file:
      - './config/hadoop.env'
    environment:
      SERVICE_PRECONDITION: "namenode:9870"

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, the Resource Manager is a key component of the YARN (Yet Another ResourceNegotiator)
  # framework. It is responsible for managing the allocation of computing resources in a Hadoop cluster,
  # such as CPU, memory, and disk, to various applications running on the cluster.
  #
  # The Resource Manager communicates with NodeManagers, which run on each machine in the cluster
  # and manage the actual resources on that machine. The Resource Manager receives resource requests
  # from applications running on the cluster and negotiates with the NodeManagers to allocate the
  # necessary resources to each application. It also monitors the resource usage of each application
  # and dynamically adjusts the resource allocation as needed.
  #
  # The Resource Manager also provides a web-based user interface for monitoring the status of
  # applications running on the cluster and their resource usage. It can also be configured to
  # use various scheduling policies, such as fair scheduling or capacity scheduling, to allocate
  # resources to applications.
  # ----------------------------------------------------------------------------------------------------
  resourcemanager:
    image: 'bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8'
    container_name: 'yarn'
    ports:
      - '8088:8088'
    depends_on:
      - 'namenode'
      - 'datanode1'
      - 'datanode2'
    networks:
      - 'hbase'
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864"
    env_file:
      - './config/hadoop.env'
    healthcheck:
      disable: true

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, a NodeManager is a component of the YARN (Yet Another Resource Negotiator) framework,
  # and it is responsible for managing the resources, such as CPU, memory, and disk, on an individual
  # node in the Hadoop cluster.
  #
  # Each machine in the cluster runs a NodeManager, and it communicates with the Resource Manager to
  # obtain the resource allocation for that node. It is responsible for managing the containers that
  # run on that node, which are the units of resource allocation for YARN. The NodeManager launches
  # and monitors the containers, and it communicates with the Resource Manager to request additional
  # resources or release unused resources as needed.
  #
  # The NodeManager is also responsible for monitoring the health of the node, such as the disk usage
  # and the number of running processes, and it reports this information to the Resource Manager. If
  # a NodeManager fails or becomes unavailable, the Resource Manager will detect the failure and
  # redistribute the containers running on that node to other available nodes in the cluster.
  # ----------------------------------------------------------------------------------------------------
  nodemanager:
    image: 'bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8'
    ports:
      - '8042:8042'
    container_name: 'nodemanager'
    depends_on:
      - 'namenode'
      - 'datanode1'
      - 'datanode2'
      - 'resourcemanager'
    networks:
      - 'hbase'
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 resourcemanager:8088"
    env_file:
      - './config/hadoop.env'

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, the History Server is a component of the Hadoop MapReduce framework that provides a
  # web-based user interface for accessing the logs and job history of completed MapReduce jobs in
  # the Hadoop cluster.
  #
  # When a MapReduce job completes, the output is written to the Hadoop Distributed File System
  # (HDFS), along with detailed logs of the job execution. The History Server provides a user
  # interface for accessing this information and analyzing the performance of completed jobs.
  #
  # The History Server stores the job history information in a database, which can be queried
  # using the web-based user interface. The user interface provides information about the input
  # and output of each job, as well as detailed information about the execution of each task in
  # the job. It also provides charts and graphs for visualizing the performance of the job, such
  # as the time taken for each task and the resource usage of each task.
  # ----------------------------------------------------------------------------------------------------
  historyserver:
    image: 'bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8'
    container_name: 'historyserver'
    ports:
      - '8188:8188'
    networks:
      - 'hbase'
    volumes:
      - 'hadoop_historyserver:/hadoop/yarn/timeline'
    depends_on:
      - 'namenode'
      - 'datanode1'
      - 'datanode2'
      - 'resourcemanager'
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 resourcemanager:8088"
    env_file:
      - './config/hadoop.env'

  # ----------------------------------------------------------------------------------------------------
  # ----------------------------------------------------------------------------------------------------
  spark:
    image: 'uhopper/hadoop-spark'
    hostname: 'spark'
    container_name: 'spark'
    domainname: 'hadoop'
    container_name: 'spark'
    networks:
      - 'hbase'
    environment:
      - 'CORE_CONF_fs_defaultFS=hdfs://namenode:8020'
      - 'YARN_CONF_yarn_resourcemanager_hostname=resourcemanager'
    command: 'tail -f /var/log/dmesg'
    depends_on:
      - 'namenode'
