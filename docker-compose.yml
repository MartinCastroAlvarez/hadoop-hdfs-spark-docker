version: '3'

volumes:
  datanode:
  namenode:
  hadoop_historyserver:
  kafka:

networks:
  hbase:
    external:
      name: 'hbase'

services:

  # ----------------------------------------------------------------------------------------------------
  # ZooKeeper is a centralized service for maintaining configuration information, naming, providing
  # distributed synchronization, and providing group services. All of these kinds of services are
  # used in some form or another by distributed applications. Each time they are implemented there
  # is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because
  # of the difficulty of implementing these kinds of services, applications initially usually skimp
  # on them, which make them brittle in the presence of change and difficult to manage. Even when
  # done correctly, different implementations of these services lead to management complexity when
  # the applications are deployed.
  # ----------------------------------------------------------------------------------------------------
  zookeeper:
    image: 'wurstmeister/zookeeper'
    networks:
      - 'hbase'
    container_name: 'zookeeper'
    ports:
      - '2181:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  # ----------------------------------------------------------------------------------------------------
  # Kafka is an open-source distributed event streaming platform originally developed by LinkedIn,
  # now part of the Apache Software Foundation. It is designed to handle real-time data feeds, with
  # a focus on fault tolerance, high throughput, and low latency.
  #
  # At a high level, Kafka allows producers to write streams of records to a set of topics, which
  # are partitioned and distributed across a cluster of nodes. Consumers can then read from one or
  # more topics and process the records in real time. Kafka is horizontally scalable, meaning that
  # it can handle large volumes of data by adding more nodes to the cluster.
  # ----------------------------------------------------------------------------------------------------
  kafka:
    image: 'confluentinc/cp-kafka:7.3.2'
    hostname: 'kafka'
    container_name: 'kafka'
    networks:
      - 'hbase'
    ports:
      - '9092:9092'
      - '19092:19092'
      - '29092:29092'
    depends_on:
      - 'zookeeper'
    environment:
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka:29092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:19092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: 'kafka.controller=DEBUG,kafka.producer.async.DefaultEventHandler=DEBUG,state.change.logger=DEBUG'
      KAFKA_AUTHORIZER_CLASS_NAME: 'kafka.security.authorizer.AclAuthorizer'
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: 'true'
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    depends_on:
      - 'zookeeper'

  # ----------------------------------------------------------------------------------------------------
  # Kafka Schema Registry is a centralized service that manages the schema of messages sent through
  # Apache Kafka. It provides a way for producers and consumers to agree on a specific schema for
  # their data, which allows for interoperability between different systems and applications. The
  # Schema Registry ensures that all messages conform to the specified schema, which helps prevent
  # data inconsistencies and errors. It also allows for versioning of schemas, so that changes can
  # be made to the schema without breaking existing applications. The Schema Registry is commonly
  # used in conjunction with Apache Kafka and is a critical component of many real-time data
  # processing pipelines. Overall, the Kafka Schema Registry is an important tool for ensuring
  # data consistency and interoperability in complex data architectures.
  # ----------------------------------------------------------------------------------------------------
  kafka-schema-registry:
    image: 'confluentinc/cp-schema-registry:5.2.1'
    container_name: 'kafka-schema-registry'
    hostname: 'kafka-schema-registry'
    networks:
      - 'hbase'
    ports:
      - '8081:8081'
    environment:
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'PLAINTEXT://kafka:29092'
      SCHEMA_REGISTRY_HOST_NAME: 'kafka-schema-registry'
      SCHEMA_REGISTRY_LISTENERS: 'http://0.0.0.0:8081'
    depends_on:
      - 'zookeeper'
      - 'kafka'

  # ----------------------------------------------------------------------------------------------------
  # Kafka Connect is a framework and set of APIs for integrating Kafka with external data sources
  # and sinks. It provides a scalable and fault-tolerant way to move data between Kafka and other
  # systems, such as databases, file systems, and message queues. Kafka Connect consists of two main
  # components: source connectors and sink connectors. Source connectors allow Kafka to consume data
  # from external systems and publish it to a Kafka topic, while sink connectors allow Kafka to push
  # data from a Kafka topic to an external system. Kafka Connect provides an easy-to-use interface
  # for configuring, deploying, and monitoring connectors. It also supports various transformation
  # and serialization formats, such as JSON, Avro, and Protobuf. Kafka Connect is commonly used in
  # data integration scenarios, such as replicating data between different databases or streaming
  # data from a web service to a Kafka topic. Overall, Kafka Connect is a valuable tool for building
  # scalable and flexible data pipelines that integrate with Apache Kafka.
  # ----------------------------------------------------------------------------------------------------
  kafka-connect:
    image: 'confluentinc/cp-kafka-connect:5.2.1'
    hostname: 'kafka-connect'
    container_name: 'kafka-connect'
    ports:
      - '8083:8083'
    networks:
      - 'hbase'
    volumes:
      - 'kafka:/etc/kafka-connect/jars/'
    depends_on:
      - 'zookeeper'
      - 'kafka'
      - 'kafka-schema-registry'
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka:29092'
      CONNECT_GROUP_ID: 'kafka-connect-my-topic'
      CONNECT_CONNECTOR_CLASS: 'com.cloudera.dim.kafka.connect.hdfs.HdfsSinkConnector'
      CONNECT_TASKS_MAX: 1
      CONNECT_REST_PORT: 8083
      CONNECT_REST_ADVERTISED_HOST_NAME: 'kafka-connect'
      CONNECT_CONFIG_STORAGE_TOPIC: 'my-topic-config'
      CONNECT_OFFSET_STORAGE_TOPIC: 'my-topic-offsets'
      CONNECT_STATUS_STORAGE_TOPIC: 'my-topic-status'
      CONNECT_KEY_CONVERTER: 'io.confluent.connect.avro.AvroConverter'
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://kafka-schema-registry:8081'
      CONNECT_VALUE_CONVERTER: 'io.confluent.connect.avro.AvroConverter'
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://kafka-schema-registry:8081'
      CONNECT_VALUE_CONVERTER_PASSTHROUGH_ENABLED: 'true'
      CONNECT_TOPICS: 'my_topic'
      CONNECT_OUTPUT_WRITER: 'com.cloudera.dim.kafka.connect.hdfs.avro.AvroPartitionWriter'
      CONNECT_OUTPUT_AVRO_PASSTHROUGH_ENABLED: 'true'
      CONNECT_LOG4J_ROOT_LOGLEVEL: 'ERROR'
      CONNECT_LOG4J_LOGGERS: 'org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR'
      CONNECT_INTERNAL_KEY_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_INTERNAL_VALUE_CONVERTER: 'org.apache.kafka.connect.json.JsonConverter'
      CONNECT_HDFS_URI: 'hdfs://namenode:9870'
      CONNECT_HDFS_OUTPUT: '/my_topic_output/'
