version: '3'

volumes:
  datanode:
  namenode:
  hadoop_historyserver:

networks:
  hbase:
    external:
      name: 'hbase'

services:

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, the NameNode is a key component of the Hadoop Distributed File System (HDFS).
  # It is responsible for managing the file system namespace and regulating access to files
  # by clients. The NameNode is a centralized component that runs on a dedicated machine in
  # the cluster, and it maintains the metadata about the files stored in HDFS, such as the
  # file name, directory structure, and the location of blocks that make up the file.
  #
  # The NameNode stores this metadata in memory for fast access, and it also persists it on
  # disk in the form of two files: fsimage and edits. The fsimage file contains a snapshot
  # of the file system metadata, and the edits file contains a log of all the changes that
  # have been made to the metadata since the last snapshot. Together, these files form a
  # checkpoint of the file system state that can be used to recover the metadata in case of a failure.
  #
  # When a client wants to read or write a file in HDFS, it first contacts the NameNode to
  # obtain information about the file, such as its location and the block IDs that make up
  # the file. The NameNode then returns this information to the client, which can then communicate
  # directly with the DataNodes that store the blocks.
  # ----------------------------------------------------------------------------------------------------
  namenode:
    image: 'bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8'
    container_name: 'namenode'
    ports:
      - '9870:9870'
      - '9000:9000'
    networks:
      - 'hbase'
    volumes:
      - 'namenode:/hadoop/dfs/name'
    environment:
      - 'CLUSTER_NAME=test'
    env_file:
      - './config/hadoop.env'
    deploy:
      mode: 'replicated'
      replicas: 1
      restart_policy:
        condition: 'on-failure'
      placement:
        constraints:
          - node.hostname == akswnc4.aksw.uni-leipzig.de

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, a DataNode is a component of the Hadoop Distributed File System (HDFS) that stores
  # the actual data in the form of blocks. The DataNode is responsible for reading and writing data
  # from the local file system, and for communicating with other DataNodes and the NameNode to manage
  # the data stored in the cluster.
  #
  # Each DataNode in the HDFS cluster stores a subset of the blocks that make up the files in the
  # file system. When a client wants to read or write a file, it first contacts the NameNode to
  # obtain the locations of the blocks that make up the file. The client can then read or write
  # the data directly from the DataNodes that store the blocks.
  #
  # DataNodes are designed to run on commodity hardware and can be added or removed from the cluster
  # as needed to scale the storage capacity of the HDFS cluster. The HDFS architecture is designed
  # to be fault-tolerant, so when a DataNode fails or becomes unavailable, the NameNode automatically
  # replicates the blocks that were stored on the failed DataNode to other DataNodes in the cluster
  # to ensure that the data is still available.
  # ----------------------------------------------------------------------------------------------------
  datanode1:
    image: 'bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8'
    container_name: 'datenode1'
    depends_on:
      - 'namenode'
    ports:
      - '9864:9864'
    networks:
      - 'hbase'
    volumes:
      - 'datanode:/hadoop/dfs/data1'
    env_file:
      - './config/hadoop.env'
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    deploy:
      mode: 'global'
      restart_policy:
        condition: 'on-failure'
  datanode2:
    image: 'bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8'
    container_name: 'datenode2'
    ports:
      - '9865:9864'
    networks:
      - 'hbase'
    volumes:
      - 'datanode:/hadoop/dfs/data2'
    env_file:
      - './config/hadoop.env'
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    deploy:
      mode: 'global'
      restart_policy:
        condition: 'on-failure'

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, the Resource Manager is a key component of the YARN (Yet Another ResourceNegotiator)
  # framework. It is responsible for managing the allocation of computing resources in a Hadoop cluster,
  # such as CPU, memory, and disk, to various applications running on the cluster.
  #
  # The Resource Manager communicates with NodeManagers, which run on each machine in the cluster
  # and manage the actual resources on that machine. The Resource Manager receives resource requests
  # from applications running on the cluster and negotiates with the NodeManagers to allocate the
  # necessary resources to each application. It also monitors the resource usage of each application
  # and dynamically adjusts the resource allocation as needed.
  #
  # The Resource Manager also provides a web-based user interface for monitoring the status of
  # applications running on the cluster and their resource usage. It can also be configured to
  # use various scheduling policies, such as fair scheduling or capacity scheduling, to allocate
  # resources to applications.
  # ----------------------------------------------------------------------------------------------------
  resourcemanager:
    image: 'bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8'
    container_name: 'yarn'
    ports:
      - '8088:8088'
    depends_on:
      - 'namenode'
      - 'datanode1'
      - 'datanode2'
    networks:
      - 'hbase'
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864"
    env_file:
      - './config/hadoop.env'
    deploy:
      mode: 'replicated'
      replicas: 1
      restart_policy:
        condition: 'on-failure'
      placement:
        constraints:
          - node.hostname == akswnc4.aksw.uni-leipzig.de
    healthcheck:
      disable: true

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, a NodeManager is a component of the YARN (Yet Another Resource Negotiator) framework,
  # and it is responsible for managing the resources, such as CPU, memory, and disk, on an individual
  # node in the Hadoop cluster.
  #
  # Each machine in the cluster runs a NodeManager, and it communicates with the Resource Manager to
  # obtain the resource allocation for that node. It is responsible for managing the containers that
  # run on that node, which are the units of resource allocation for YARN. The NodeManager launches
  # and monitors the containers, and it communicates with the Resource Manager to request additional
  # resources or release unused resources as needed.
  #
  # The NodeManager is also responsible for monitoring the health of the node, such as the disk usage
  # and the number of running processes, and it reports this information to the Resource Manager. If
  # a NodeManager fails or becomes unavailable, the Resource Manager will detect the failure and
  # redistribute the containers running on that node to other available nodes in the cluster.
  # ----------------------------------------------------------------------------------------------------
  nodemanager:
    image: 'bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8'
    ports:
      - '8042:8042'
    container_name: 'nodemanager'
    depends_on:
      - 'namenode'
      - 'datanode1'
      - 'datanode2'
      - 'resourcemanager'
    networks:
      - 'hbase'
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 resourcemanager:8088"
    env_file:
      - './config/hadoop.env'
    deploy:
      mode: 'global'
      restart_policy:
        condition: 'on-failure'

  # ----------------------------------------------------------------------------------------------------
  # In Hadoop, the History Server is a component of the Hadoop MapReduce framework that provides a
  # web-based user interface for accessing the logs and job history of completed MapReduce jobs in
  # the Hadoop cluster.
  #
  # When a MapReduce job completes, the output is written to the Hadoop Distributed File System
  # (HDFS), along with detailed logs of the job execution. The History Server provides a user
  # interface for accessing this information and analyzing the performance of completed jobs.
  #
  # The History Server stores the job history information in a database, which can be queried
  # using the web-based user interface. The user interface provides information about the input
  # and output of each job, as well as detailed information about the execution of each task in
  # the job. It also provides charts and graphs for visualizing the performance of the job, such
  # as the time taken for each task and the resource usage of each task.
  # ----------------------------------------------------------------------------------------------------
  historyserver:
    image: 'bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8'
    container_name: 'historyserver'
    ports:
      - '8188:8188'
    networks:
      - 'hbase'
    volumes:
      - 'hadoop_historyserver:/hadoop/yarn/timeline'
    depends_on:
      - 'namenode'
      - 'datanode1'
      - 'datanode2'
      - 'resourcemanager'
    environment:
      SERVICE_PRECONDITION: "namenode:9870 datanode1:9864 datanode2:9864 resourcemanager:8088"
    env_file:
      - './config/hadoop.env'
    deploy:
      mode: 'replicated'
      replicas: 1
      placement:
        constraints:
          - node.hostname == akswnc4.aksw.uni-leipzig.de

  # ----------------------------------------------------------------------------------------------------
  # ZooKeeper is a centralized service for maintaining configuration information, naming, providing
  # distributed synchronization, and providing group services. All of these kinds of services are
  # used in some form or another by distributed applications. Each time they are implemented there
  # is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because
  # of the difficulty of implementing these kinds of services, applications initially usually skimp
  # on them, which make them brittle in the presence of change and difficult to manage. Even when
  # done correctly, different implementations of these services lead to management complexity when
  # the applications are deployed.
  # ----------------------------------------------------------------------------------------------------
  zookeeper:
    image: 'wurstmeister/zookeeper'
    networks:
      - 'hbase'
    container_name: 'zookeeper'
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  # ----------------------------------------------------------------------------------------------------
  # Kafka is an open-source distributed event streaming platform originally developed by LinkedIn,
  # now part of the Apache Software Foundation. It is designed to handle real-time data feeds, with
  # a focus on fault tolerance, high throughput, and low latency.
  #
  # At a high level, Kafka allows producers to write streams of records to a set of topics, which
  # are partitioned and distributed across a cluster of nodes. Consumers can then read from one or
  # more topics and process the records in real time. Kafka is horizontally scalable, meaning that
  # it can handle large volumes of data by adding more nodes to the cluster.
  # ----------------------------------------------------------------------------------------------------
  kafka1:
    image: 'confluentinc/cp-kafka:7.3.2'
    hostname: 'kafka1'
    container_name: 'kafka1'
    networks:
      - 'hbase'
    ports:
      - "9092:9092"
      - "29092:29092"
    depends_on:
      - 'zookeeper'
    environment:
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka1:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: 'kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO'
      KAFKA_AUTHORIZER_CLASS_NAME: 'kafka.security.authorizer.AclAuthorizer'
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: 'true'
    depends_on:
      - 'zookeeper'
  kafka2:
    image: 'confluentinc/cp-kafka:7.3.2'
    hostname: 'kafka2'
    container_name: 'kafka2'
    networks:
      - 'hbase'
    ports:
      - "9093:9093"
      - "29093:29093"
    depends_on:
      - 'zookeeper'
    environment:
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka2:19093,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9093,DOCKER://host.docker.internal:29093'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_BROKER_ID: 2
      KAFKA_LOG4J_LOGGERS: 'kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO'
      KAFKA_AUTHORIZER_CLASS_NAME: 'kafka.security.authorizer.AclAuthorizer'
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: 'true'
